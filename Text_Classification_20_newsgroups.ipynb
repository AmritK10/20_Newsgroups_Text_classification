{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification_project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ETKSGEGU_EYU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing modules\n",
        "import os\n",
        "import codecs\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jGq78nAZr9ND",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ]
    },
    {
      "metadata": {
        "id": "4e2pi7u6CH39",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#downloading and extracting the files on colab server\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve (\"https://archive.ics.uci.edu/ml/machine-learning-databases/20newsgroups-mld/20_newsgroups.tar.gz\", \"a.tar.gz\")\n",
        "import tarfile\n",
        "tar = tarfile.open(\"a.tar.gz\")\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4k_0llvo_EYX",
        "colab_type": "code",
        "outputId": "22bad58e-b34a-43d9-b6e5-d2d0d9a812b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#making a list of all the file paths and their corresponding class\n",
        "f_paths=[]\n",
        "i=-1\n",
        "path=\"20_newsgroups\"\n",
        "folderlist=os.listdir(path)\n",
        "if \".DS_Store\" in folderlist:\n",
        "  folderlist.remove('.DS_Store')\n",
        "for folder in folderlist:\n",
        "  i+=1\n",
        "  filelist=os.listdir(path+'/'+folder)\n",
        "  for file in filelist:\n",
        "    f_paths.append((path+'/'+folder+'/'+file,i))\n",
        "len(f_paths)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19997"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "fU1RzKP1yyPp",
        "colab_type": "code",
        "outputId": "c0975a57-4f16-4d5b-bf67-7d72f3b8ba7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#splitting the list of paths into training and testing data\n",
        "from sklearn import model_selection\n",
        "x_train,x_test=model_selection.train_test_split(f_paths)\n",
        "len(x_train),len(x_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14997, 5000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "O_8VmW_fzvO5",
        "colab_type": "code",
        "outputId": "5beb67da-20f7-4f7a-f4bf-d9e3ed5739c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#Making the lists X_train and X_test containg only the paths of the files in training and testing data\n",
        "#First making lists Y_train and Y_test containing the classes of the training and testing data\n",
        "X_train=[]\n",
        "X_test=[]\n",
        "Y_train=[]\n",
        "Y_test=[]\n",
        "for i in range(len(x_train)):\n",
        "  X_train.append(x_train[i][0])\n",
        "  Y_train.append(x_train[i][1])\n",
        "for i in range(len(x_test)):\n",
        "  X_test.append(x_test[i][0])\n",
        "  Y_test.append(x_test[i][1])\n",
        "#Transforming Y_train and Y_test into 1 dimensional np arrays\n",
        "Y_train=(np.array([Y_train])).reshape(-1)\n",
        "Y_test=(np.array([Y_test])).reshape(-1)\n",
        "#shape of Y_train and Y_test np arrays\n",
        "Y_train.shape,Y_test.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((14997,), (5000,))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "D4sbNla-Zjf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "49dc2099-d159-446d-f8bd-1bdbfdbd916b"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop=set(stopwords.words(\"english\"))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I1IWj03cZcz9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "28cafc7d-37c1-4eac-8bba-cba8fb478fc3"
      },
      "cell_type": "code",
      "source": [
        "#adding all the above lists and including punctuations to stop words\n",
        "stop_words=list(stop)+list(set(string.punctuation))\n",
        "len(stop_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "211"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "rxf9J4g4_EYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#making vocabulary from the files in X_train i.e. training data\n",
        "vocab={}\n",
        "count =0\n",
        "for filename in X_train:\n",
        "  count+=1\n",
        "  f = open(filename,'r',errors='ignore')\n",
        "  record=f.read()\n",
        "  words=record.split()\n",
        "  for word in words:\n",
        "    if len(word)>2:\n",
        "      if word.lower() not in stop_words:\n",
        "        if word.lower() in vocab:\n",
        "          vocab[word.lower()]+=1\n",
        "        else:\n",
        "          vocab[word.lower()]=1\n",
        "  f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rl5HJz0-_EYk",
        "colab_type": "code",
        "outputId": "aeab1391-fe2a-475f-8bbf-aba0943b9910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#length of the vocabulary\n",
        "len(vocab)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "354297"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "w0G8_FDc_EYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#sorting the vocabulary on the basis of the frequency of the word\n",
        "#making the sorted vocabulary\n",
        "import operator\n",
        "sorted_vocab = sorted(vocab.items(), key= operator.itemgetter(1), reverse= True)   # sort the vocab based on frequency"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AoK5V6zY_EYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#making the list feature_names containg the words with the frequency of the top 2000 words\n",
        "feature_names = []\n",
        "for i in range(len(sorted_vocab)):\n",
        "    if(sorted_vocab[2000][1] <= sorted_vocab[i][1]):\n",
        "        feature_names.append(sorted_vocab[i][0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iUuVWVOO_EYr",
        "colab_type": "code",
        "outputId": "bcdaef8d-6962-4381-c647-9ca110191733",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#length of the feature_names i.e. number of our features\n",
        "print(len(feature_names))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5LvFD1r8_EYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#making dataframes df_train and df_test with columns having the feature names i.e. the words\n",
        "df_train=pd.DataFrame(columns=feature_names)\n",
        "df_test=pd.DataFrame(columns=feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v5GbdNAY_EYw",
        "colab_type": "code",
        "outputId": "831eefb5-e743-4bfd-8153-0cd8367c93ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "count_train,count_test=0,0\n",
        "\n",
        "#transforming each file in X_train into a row in the dataframe df_train having columns as feature names and values as the frequency of that feature name i.e that word\n",
        "for filename in X_train:\n",
        "  count_train+=1\n",
        "  #adding a row of zeros for each file\n",
        "  df_train.loc[len(df_train)]=np.zeros(len(feature_names))\n",
        "  f = open(filename,'r',errors='ignore')\n",
        "  record=f.read()\n",
        "  words=record.split()\n",
        "  #parsing through all the words of the file\n",
        "  for word in words:\n",
        "    if word.lower() in df_train.columns:\n",
        "      df_train[word.lower()][len(df_train)-1]+=1 #if the word is in the column names then adding 1 to the frequency of that word in the row\n",
        "  f.close()\n",
        "  \n",
        "#transforming each file in X_test into a row in the dataframe df_test having columns as feature names and values as the frequency of that feature name i.e that word  \n",
        "for filename in X_test:\n",
        "  count_test+=1\n",
        "  #adding a row of zeros for each file\n",
        "  df_test.loc[len(df_test)]=np.zeros(len(feature_names))\n",
        "  f = open(filename,'r',errors='ignore')\n",
        "  record=f.read()\n",
        "  words=record.split()\n",
        "  #parsing through all the words of the file\n",
        "  for word in words:\n",
        "    if word.lower() in df_test.columns:\n",
        "      df_test[word.lower()][len(df_test)-1]+=1 #if the word is in the column names then adding 1 to the frequency of that word in the row\n",
        "  f.close()\n",
        "  \n",
        "#printing the number files tranformed in training and testing data\n",
        "print(count_train,count_test)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14997 5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u2LneYsIdbyy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#putting the values of the datafames into X_train and X_test\n",
        "X_train=df_train.values\n",
        "X_test=df_test.values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nFKQk-p_pLUl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Using the inbuilt Multinomial Naive Bayes classifier from sklearn**"
      ]
    },
    {
      "metadata": {
        "id": "TMX2648a_EYz",
        "colab_type": "code",
        "outputId": "00ef1c1d-bb7e-4b09-b928-ac48062288cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report,confusion_matrix\n",
        "clf=MultinomialNB()\n",
        "#fitting the classifier on training data\n",
        "clf.fit(X_train,Y_train)\n",
        "#prediciting the classes of the testing data\n",
        "Y_pred=clf.predict(X_test)\n",
        "#classification report\n",
        "print(classification_report(Y_test,Y_pred))\n",
        "#testing score\n",
        "print(\"Testing: \",clf.score(X_test,Y_test))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85       243\n",
            "           1       0.79      0.71      0.75       256\n",
            "           2       0.95      0.87      0.90       260\n",
            "           3       0.95      0.99      0.97       231\n",
            "           4       0.95      0.75      0.84       253\n",
            "           5       0.94      0.95      0.95       233\n",
            "           6       0.65      0.51      0.57       252\n",
            "           7       0.82      0.89      0.85       225\n",
            "           8       0.87      0.80      0.84       241\n",
            "           9       0.81      0.91      0.86       251\n",
            "          10       0.83      0.80      0.81       245\n",
            "          11       0.98      0.94      0.96       261\n",
            "          12       0.75      0.79      0.77       268\n",
            "          13       0.70      0.85      0.77       254\n",
            "          14       0.83      0.81      0.82       242\n",
            "          15       0.90      0.84      0.87       275\n",
            "          16       0.71      0.54      0.61       235\n",
            "          17       0.87      0.94      0.90       285\n",
            "          18       0.65      0.90      0.76       251\n",
            "          19       0.69      0.85      0.76       239\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      5000\n",
            "   macro avg       0.83      0.82      0.82      5000\n",
            "weighted avg       0.83      0.82      0.82      5000\n",
            "\n",
            "Testing:  0.8216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bjXYNE_2Mk4f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Self implemented Multinomial Naive Bayes**"
      ]
    },
    {
      "metadata": {
        "id": "aoMoTUv4MvGx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#makes the nested dictionary required for NB using the training data\n",
        "def fit(X,Y):\n",
        "    dictionary={}\n",
        "    y_classes=set(Y)\n",
        "    #iterating over each class of y\n",
        "    for y_class in y_classes:\n",
        "        #adding the class as a key to the dictionary\n",
        "        dictionary[y_class]={}\n",
        "        n_features=X.shape[1]\n",
        "        rows=(Y==y_class)\n",
        "        #making the arrays having only those rows where class is y_class\n",
        "        X_y_class=X[rows]\n",
        "        Y_y_class=Y[rows]\n",
        "        #adding the total number of files as total_data\n",
        "        dictionary[\"total_data\"]=X.shape[0]\n",
        "        #iterating over each feature\n",
        "        for i in range(n_features):\n",
        "            #adding the feature as a key which has the count of that word in Y=y_class as its value\n",
        "            dictionary[y_class][i]=X_y_class[:,i].sum()\n",
        "            #adding the total number of files as total_class\n",
        "            dictionary[y_class][\"total_class\"]=X_y_class.shape[0]\n",
        "            #adding the sum of all the words in Y=y_class i.e. total no. of words in Y=y_class\n",
        "            dictionary[y_class][\"total_words\"]=X_y_class.sum()\n",
        "    return dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e-mb2wI7kzVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#calculates the probability of the feature vector belonging to a particular class and the probability of the class\n",
        "#returns the product of the above 2 probabilities\n",
        "def probability(x,dictionary,y_class):\n",
        "    #output intially has probability of the particular class in log terms\n",
        "    output=np.log(dictionary[y_class][\"total_class\"])-np.log(dictionary[\"total_data\"])\n",
        "    n_features=len(dictionary[y_class].keys())-2\n",
        "    #calculates probability of x being in a particular class by calulating probability of each word being in that class\n",
        "    for i in range(n_features):\n",
        "        if x[i]>0:\n",
        "            #probability of the ith word being in this class in terms of log\n",
        "            p_i=x[i]*(np.log(dictionary[y_class][i] + 1) - np.log(dictionary[y_class][\"total_words\"]+n_features))\n",
        "            output+=p_i\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_jZFhRcAk11l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#predicts the class to which a single file feature vector belongs to\n",
        "def predictSinglePoint(x,dictionary):\n",
        "    classes=dictionary.keys()\n",
        "    #contains the class having the max probability\n",
        "    best_class=1\n",
        "    #max probability\n",
        "    best_prob=-1000\n",
        "    first=True\n",
        "    #iterating over all the classes\n",
        "    for y_class in classes:\n",
        "        if y_class==\"total_data\":\n",
        "            continue\n",
        "        #finding probability of this file feature vector belonging to y_class\n",
        "        p_class=probability(x,dictionary,y_class)\n",
        "        if(first or p_class>best_prob):\n",
        "            best_prob=p_class\n",
        "            best_class=y_class\n",
        "        first=False\n",
        "    return best_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K7mf0zjek4mK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#predicts the classes to which all the file feature vectors belong in the testing data\n",
        "def predict(X_test,dictionary):\n",
        "    y_pred=[]\n",
        "    #iterates over all the file feature vectors\n",
        "    for x in X_test:\n",
        "        #predicts the class of a particular file feature vector\n",
        "        x_class=predictSinglePoint(x,dictionary)\n",
        "        y_pred.append(x_class)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bB8CnTXGk7mh",
        "colab_type": "code",
        "outputId": "117053df-b940-4308-9539-a7c3446647d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "dictionary=fit(X_train,Y_train) #makes the required dictionary\n",
        "y_pred=predict(X_test,dictionary)# predicts the classes\n",
        "print(classification_report(Y_test,y_pred)) #classification report for testing data"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85       243\n",
            "           1       0.79      0.71      0.75       256\n",
            "           2       0.95      0.87      0.90       260\n",
            "           3       0.95      0.99      0.97       231\n",
            "           4       0.95      0.75      0.84       253\n",
            "           5       0.94      0.95      0.95       233\n",
            "           6       0.65      0.51      0.57       252\n",
            "           7       0.82      0.89      0.85       225\n",
            "           8       0.87      0.80      0.84       241\n",
            "           9       0.81      0.91      0.86       251\n",
            "          10       0.83      0.80      0.81       245\n",
            "          11       0.98      0.94      0.96       261\n",
            "          12       0.75      0.79      0.77       268\n",
            "          13       0.70      0.85      0.77       254\n",
            "          14       0.83      0.81      0.82       242\n",
            "          15       0.90      0.84      0.87       275\n",
            "          16       0.71      0.54      0.61       235\n",
            "          17       0.87      0.94      0.90       285\n",
            "          18       0.65      0.90      0.76       251\n",
            "          19       0.69      0.85      0.76       239\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      5000\n",
            "   macro avg       0.83      0.82      0.82      5000\n",
            "weighted avg       0.83      0.82      0.82      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "djwGLp_BBpjl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Comparison of results between inbuilt and self implemented Multinomial NB**"
      ]
    },
    {
      "metadata": {
        "id": "en7AriSaBBEe",
        "colab_type": "code",
        "outputId": "bfb775bc-9c2a-4139-ac4f-8c22b55c206b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1037
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(\"Classification report for inbuilt Multinomial NB on testing data: \")\n",
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(classification_report(Y_test,Y_pred))\n",
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(\"Classification report for self implemented Multinomial NB on testing data: \")\n",
        "print(\"----------------------------------------------------------------------------\")\n",
        "print(classification_report(Y_test,y_pred))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------------------\n",
            "Classification report for inbuilt Multinomial NB on testing data: \n",
            "----------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85       243\n",
            "           1       0.79      0.71      0.75       256\n",
            "           2       0.95      0.87      0.90       260\n",
            "           3       0.95      0.99      0.97       231\n",
            "           4       0.95      0.75      0.84       253\n",
            "           5       0.94      0.95      0.95       233\n",
            "           6       0.65      0.51      0.57       252\n",
            "           7       0.82      0.89      0.85       225\n",
            "           8       0.87      0.80      0.84       241\n",
            "           9       0.81      0.91      0.86       251\n",
            "          10       0.83      0.80      0.81       245\n",
            "          11       0.98      0.94      0.96       261\n",
            "          12       0.75      0.79      0.77       268\n",
            "          13       0.70      0.85      0.77       254\n",
            "          14       0.83      0.81      0.82       242\n",
            "          15       0.90      0.84      0.87       275\n",
            "          16       0.71      0.54      0.61       235\n",
            "          17       0.87      0.94      0.90       285\n",
            "          18       0.65      0.90      0.76       251\n",
            "          19       0.69      0.85      0.76       239\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      5000\n",
            "   macro avg       0.83      0.82      0.82      5000\n",
            "weighted avg       0.83      0.82      0.82      5000\n",
            "\n",
            "----------------------------------------------------------------------------\n",
            "Classification report for self implemented Multinomial NB on testing data: \n",
            "----------------------------------------------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.80      0.85       243\n",
            "           1       0.79      0.71      0.75       256\n",
            "           2       0.95      0.87      0.90       260\n",
            "           3       0.95      0.99      0.97       231\n",
            "           4       0.95      0.75      0.84       253\n",
            "           5       0.94      0.95      0.95       233\n",
            "           6       0.65      0.51      0.57       252\n",
            "           7       0.82      0.89      0.85       225\n",
            "           8       0.87      0.80      0.84       241\n",
            "           9       0.81      0.91      0.86       251\n",
            "          10       0.83      0.80      0.81       245\n",
            "          11       0.98      0.94      0.96       261\n",
            "          12       0.75      0.79      0.77       268\n",
            "          13       0.70      0.85      0.77       254\n",
            "          14       0.83      0.81      0.82       242\n",
            "          15       0.90      0.84      0.87       275\n",
            "          16       0.71      0.54      0.61       235\n",
            "          17       0.87      0.94      0.90       285\n",
            "          18       0.65      0.90      0.76       251\n",
            "          19       0.69      0.85      0.76       239\n",
            "\n",
            "   micro avg       0.82      0.82      0.82      5000\n",
            "   macro avg       0.83      0.82      0.82      5000\n",
            "weighted avg       0.83      0.82      0.82      5000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}